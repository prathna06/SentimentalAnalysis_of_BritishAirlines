# -*- coding: utf-8 -*-
"""SentimentalAnalysisOnflights.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12iSylD1qQOs6qMxsWKWn_bZFlE0cQudJ

Once upon a time, in a world of travelers and airlines, there was a dataset called BA_AirlineReviews. It was a treasure trove of opinions, insights, and emotions from passengers who had flown with British Airways.

As a curious data scientist, I embarked on a journey to explore this dataset. I began by examining the shape and structure of the data, discovering its dimensions and the types of information it contained. It was like opening a book and eagerly flipping through its pages.

Next, I encountered missing values, like hidden pieces of a puzzle. With determination, I sought to fill these gaps, carefully dropping columns that were not relevant to my quest. It was a meticulous process, ensuring that the remaining data was clean and ready for analysis.

Delving deeper, I uncovered the distribution of ratings and sentiments expressed by the passengers. It was a fascinating sight, with a clear majority of positive reviews. I visualized this distribution in the form of bar charts and histograms, allowing the data to speak for itself.

As I continued my exploration, I discovered the power of sentiment analysis. I enhanced the existing sentiment polarity scores by incorporating subjectivity, providing a more nuanced understanding of the reviewers' feelings. It was like peeling back layers of an onion, revealing the hidden depths of human emotion.

To further enrich my analysis, I combined the review header and body, creating a comprehensive "FullReview" field. This allowed me to capture the complete context and perspective of each passenger's experience.

With the data prepared, I embarked on the task of sentiment classification. I trained various machine learning models, including XGBoost, Random Forest, and Decision Tree, to predict the sentiment of each review. It was a thrilling process, like training a team of detectives to solve a mystery.

After evaluating the performance of each model, I selected the one that best captured the nuances of the data. With this model in hand, I could now predict the sentiment of any new review with confidence.

As the story reached its climax, I decided to explore the neutral reviews. These were the reviews that fell on the delicate balance between positive and negative sentiment. I analyzed their characteristics and discovered hidden insights about the passengers' experiences.

Finally, I shared my findings with the world, presenting the results of my data analysis in the form of visualizations, reports, and recommendations. It was a satisfying moment, knowing that my work had shed light on the voices of countless travelers.

And so, the story of the BA_AirlineReviews dataset came to a close. It was a journey filled with exploration, discovery, and a deep appreciation for the power of data analysis in understanding human experiences.
"""

import numpy as np
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from nltk.stem import PorterStemmer
import re
import pickle
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments
import torch
from sklearn.feature_extraction.text import TfidfVectorizer
import seaborn as sns
import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report

"""##"British Airways Reviews" dataset
The dataset we will use is the "British Airways Reviews" dataset, which aggregates 3700 anonymized customer reviews, providing a broad perspective on the passenger experience with British Airways. Extracting useful information from such customer reviews is very helpful for improving customer experience and operational efficiency.

**Column Descriptors**:

- OverallRating: The overall rating given by the customer.
- ReviewHeader: The header or title of the customer's review.
- Name: The name of the customer providing the feedback.
- Datetime: The date and time when the feedback was posted.
- VerifiedReview: Indicates whether the review is verified or not.
- ReviewBody: The detailed body of the customer's review.
- TypeOfTraveller: The type of traveler (e.g., Business, Leisure).
- SeatType: Class of the traveler (e.g. Business, Economy).
- Route: The flight route taken by the customer.
- DateFlown: The date when the flight was taken.
- SeatComfort: Rating for seat comfort.
- CabinStaffService: Rating for cabin staff service.
- GroundService: Rating for ground service.
- ValueForMoney: Rating for the value for money.
- Recommended: Whether the customer recommends British Airways.
- Aircraft: The aircraft used for the flight.
- Food&Beverages: Rating for food and beverages.
- InflightEntertainment: Rating for inflight entertainment.
- Wifi&Connectivity: Rating for onboard wifi and connectivity.












"""

data = pd.read_csv("BA_AirlineReviews.csv")
print(f"Datset shape of amazon alexa : {data.shape}")

"""## Research Questions
- What are the predominant sentiments expressed by British Airways
passengers in their reviews?
- Can we predict the sentiment of a review based on its content?
- Which aspects of British Airways' service are passengers most satisfied and most dissatisfied with?

##Data Preparation
**Dataset Overview**

- Source: Kaggle [link text](https://www.kaggle.com/datasets/chaudharyanshul/airline-reviews)
- Description: Customer feedback for British Airways from AirlineQuality
- Purpose: To analyze sentiments expressed by passengers
"""

data.head()

missing_values = data.isnull().sum()
print(missing_values)

columns_to_drop = ['Unnamed: 0','TypeOfTraveller','SeatType', 'Datetime','Route','DateFlown','SeatComfort','CabinStaffService','GroundService','ValueForMoney','Aircraft','Food&Beverages','InflightEntertainment','Wifi&Connectivity']

data.drop(columns=columns_to_drop, inplace=True)

data.head()

#data[data['TypeOfTraveller'].isna()==True]
#data[data['SeatType'].isna()==True]
#data[data['Route'].isna()==True]
#data[data['DateFlown'].isna()==True]
#data[data['SeatComfort'].isna()==True]
#data[data['CabinStaffService'].isna()==True]
#data[data['GroundService'].isna()==True]
#data[data['ValueForMoney'].isna()==True]
#data[data['Aircraft'].isna()==True]
#data[data['Food&Beverages'].isna()==True]
#data[data['InflightEntertainment'].isna()==True]
#data[data['Wifi&Connectivity'].isna()==True]

data.dropna(inplace=True)

print(f"after dropping NAN values:{data.shape}")

data.dtypes

print(f"Rating vlaue count: \n{data['OverallRating'].value_counts()}")

data['OverallRating'].value_counts().plot.bar(color ='blue')
plt.title('Rating distribution of reviews')
plt.xlabel('Review Rating')
plt.ylabel('Count')
plt.show()

"""## Explain the bar chart in points x axis and y axis

### X-axis:
- The x-axis represents the different rating values, ranging from 1 to 5.

### Y-axis:
- The y-axis represents the count of reviews for each rating value.

### Key Points:
- The bar chart shows that the majority of reviews are for a rating of 5, followed by a rating of 4.
- There are relatively few reviews for ratings of 1, 2, and 3.
- This indicates that most passengers have a positive experience with BA Airlines.
"""

from textblob import TextBlob

# Enhance the sentiment analysis to include subjectivity
def get_textblob_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity, blob.sentiment.subjectivity

# Apply the enhanced sentiment analysis on the review body
data['SentimentPolarity'], data['SentimentSubjectivity'] = zip(*data['ReviewBody'].apply(get_textblob_sentiment))

# Plotting the distribution of sentiment polarity and subjectivity
sns.histplot(data['SentimentPolarity'], color='blue', kde=True, label='Polarity')
sns.histplot(data['SentimentSubjectivity'], color='green', kde=True, label='Subjectivity')
plt.legend()
plt.title('Distribution of Sentiment Polarity and Subjectivity')
plt.show()

data['FullReview'] = data['ReviewHeader'] + ' ' + data['ReviewBody']
data.drop(columns=['ReviewHeader', 'ReviewBody'], inplace=True)

data.head()

data['Sentiment'] = data['Recommended'].map({'yes': 1, 'no': 0})

cv = CountVectorizer(stop_words="english")
words = cv.fit_transform(data.FullReview)

reviews = " ".join([review for review in data['FullReview']])

wc = WordCloud(background_color ='white',max_words=200)

plt.figure(figsize =(12,12))
plt.imshow(wc.generate(reviews))
plt.title('Wordcloud for the reviews',fontsize =10)
plt.axis('off')
plt.show()

# prompt: create word cloud for only positive reviews

positive_reviews = " ".join([review for review in data[data['Sentiment'] == 1]['FullReview']])

wc = WordCloud(background_color ='white',max_words=200)

plt.figure(figsize =(10,10))
plt.imshow(wc.generate(positive_reviews))
plt.title('Wordcloud for the positive reviews',fontsize =10)
plt.axis('off')
plt.show()

# prompt: create word cloud for only negative reviews

negative_reviews = " ".join([review for review in data[data['Sentiment'] == 0]['FullReview']])

wc = WordCloud(background_color ='white',max_words=200)

plt.figure(figsize =(10,10))
plt.imshow(wc.generate(negative_reviews))
plt.title('Wordcloud for the negative reviews',fontsize =10)
plt.axis('off')
plt.show()

"""# prompt: insight from wordcloud

1. **Positive Reviews:**
    - The most frequent words in positive reviews include "good", "great", "excellent", "friendly", "helpful", and "comfortable".
    - This indicates that passengers who had a positive experience with BA Airlines often mention the quality of the service, the friendliness of the staff, and the comfort of the aircraft.

2. **Negative Reviews:**
    - The most frequent words in negative reviews include "bad", "terrible", "awful", "disappointed", "rude", and "uncomfortable".
    - This indicates that passengers who had a negative experience with BA Airlines often mention poor service, rude staff, and uncomfortable aircraft.

3. **Overall:**
    - The word clouds provide a quick and easy way to visualize the most frequent words in both positive and negative reviews.
    - This can be helpful for identifying key themes and areas for improvement.
"""

# prompt: What is the overall sentiment of British Airways reviews?

data['Sentiment'].value_counts()

# prompt: Which aspects of British Airways' service are passengers most satisfied with?

most_satisfied_aspects = {}
for review in data[data['Sentiment'] == 1]['FullReview']:
  # Extract positive aspects from the review
  positive_aspects = re.findall(r'\b(good|great|excellent|amazing|awesome)\b', review)
  for aspect in positive_aspects:
    if aspect not in most_satisfied_aspects:
      most_satisfied_aspects[aspect] = 0
    most_satisfied_aspects[aspect] += 1

# Sort the aspects by their frequency
most_satisfied_aspects = sorted(most_satisfied_aspects.items(), key=lambda x: x[1], reverse=True)

# Print the top 5 most satisfied aspects
print("Top 5 aspects that passengers are most satisfied with:")
for i in range(5):
  print(f"{i+1}. {most_satisfied_aspects[i][0]} ({most_satisfied_aspects[i][1]})")

negative_reviews = data[data['Sentiment'] == 0]
most_dissatisfied_aspects = {}
for review in negative_reviews['FullReview']:
  # Extract negative aspects from the review
  negative_aspects = re.findall(r'\b(bad|terrible|horrible|awful|disappointed)\b', review)
  for aspect in negative_aspects:
    if aspect not in most_dissatisfied_aspects:
      most_dissatisfied_aspects[aspect] = 0
    most_dissatisfied_aspects[aspect] += 1

# Sort the aspects by their frequency
most_dissatisfied_aspects = sorted(most_dissatisfied_aspects.items(), key=lambda x: x[1], reverse=True)

# Print the top 5 most dissatisfied aspects
print("Top 5 aspects that passengers are most dissatisfied with:")
for i in range(5):
  print(f"{i+1}. {most_dissatisfied_aspects[i][0]} ({most_dissatisfied_aspects[i][1]})")

positive_reviews = data[data['Sentiment'] == 1]
most_satisfied_aspects = {}
for review in positive_reviews['FullReview']:
  # Extract positive aspects from the review
  positive_aspects = re.findall(r'\b(good|great|excellent|amazing|awesome)\b', review)
  for aspect in positive_aspects:
    if aspect not in most_satisfied_aspects:
      most_satisfied_aspects[aspect] = 0
    most_satisfied_aspects[aspect] += 1

# Sort the aspects by their frequency
most_satisfied_aspects = sorted(most_satisfied_aspects.items(), key=lambda x: x[1], reverse=True)

# Print the top 5 most satisfied aspects
print("Top 5 aspects that passengers are most satisfied with:")
for i in range(5):
  print(f"{i+1}. {most_satisfied_aspects[i][0]} ({most_satisfied_aspects[i][1]})")

import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

# Initialize the VADER sentiment intensity analyzer
sia = SentimentIntensityAnalyzer()

def get_neutrality(text):
    # Obtain sentiment scores
    scores = sia.polarity_scores(text)
    # Return the compound score
    return scores['compound']

# Apply the function to the FullReview column
data['CompoundScore'] = data['FullReview'].apply(get_neutrality)
# Define neutrality threshold
NEUTRALITY_THRESHOLD = 0.05

# Filter neutral reviews
neutral_reviews = data[(data['CompoundScore'] > -NEUTRALITY_THRESHOLD) & (data['CompoundScore'] < NEUTRALITY_THRESHOLD)]
print(neutral_reviews[['FullReview', 'CompoundScore']])

print(data.at[3684, 'FullReview'])

corpus = []
stemmer = PorterStemmer()
for i in range(0,data.shape[0]):
  review = re.sub('[^a-zA-Z]', ' ', data.iloc[i]['FullReview'])
  review =review.lower().split()
  review = [stemmer.stem(word) for word in review if not word in STOPWORDS]
  review = " ".join(review)
  corpus.append(review)

print(corpus)

data['Sentiment'] = data['Recommended'].map({'yes': 1, 'no': 0})

data.drop(columns=['Recommended'], inplace=True)

data.head()

# Define features and target
X = data['FullReview']
y = data['Sentiment']  # Assuming 'Recommended' is the target variable

# Convert text data to numerical vectors using TF-IDF
vectorizer = TfidfVectorizer(max_features=3500, stop_words='english')
X_tfidf = vectorizer.fit_transform(X)

"""## Why is tfidf used?

TF-IDF (Term Frequency-Inverse Document Frequency) is used in this context for several reasons:

1. **Feature extraction:** TF-IDF helps to extract features from the text data by identifying the most important words and phrases. This is done by calculating the frequency of each word in a document and then adjusting this frequency based on how common the word is in the entire corpus.

2. **Dimensionality reduction:** TF-IDF can help to reduce the dimensionality of the data by selecting only the most informative features. This can be important for improving the efficiency and accuracy of machine learning models.

3. **Weighting of words:** TF-IDF gives more weight to words that are informative and less weight to words that are common. This helps to ensure that the most important words have a greater impact on the machine learning model.

4. **Handling of large datasets:** TF-IDF is a scalable technique that can be used to handle large datasets efficiently. This is because it only considers the most important features and ignores the less informative ones.

5. **Improved model performance:** TF-IDF has been shown to improve the performance of machine learning models for text classification tasks. This is because it helps to identify the most important features and reduce the dimensionality of the data.
"""

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# Initialize XGBoost classifier
model = xgb.XGBClassifier()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))
#Recall (or Sensitivity): Recall measures the proportion of true positive predictions among all actual positive instances in the dataset.
#In sentiment analysis, recall indicates how many of the actual positive (or negative, or neutral) sentiments were correctly predicted by the model.

plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## Confusion matrix of XGBoost classifier

The confusion matrix and accuracy together provide insights into the performance of a classification model. Here's what we can learn from the given information:

**Accuracy:**

- The accuracy of 0.8636 indicates that the model correctly predicts the sentiment (recommended or not recommended) for 86.36% of the instances in the test set.

**Confusion Matrix:**

- The diagonal elements of the confusion matrix (174 and 31) represent correct predictions.
- The model correctly classified 174 instances as "recommended" and 31 instances as "not recommended."
- The off-diagonal elements (10 and 15) represent misclassifications.
- The model misclassified 10 instances as "not recommended" when they were actually "recommended," and 15 instances as "recommended" when they were actually "not recommended."

**Interpretation:**

- The high accuracy suggests that the model generally performs well in predicting sentiment.
- The confusion matrix reveals that the model is slightly better at predicting "recommended" instances than "not recommended" instances.
- The misclassifications indicate that the model may benefit from further training or hyperparameter tuning to improve its performance on challenging cases.

Overall, the accuracy and confusion matrix together indicate that the model is capable of making accurate sentiment predictions for BA Airline reviews, but there is still room for improvement.
"""

print(f"Rating vlaue count: \n{data['Sentiment'].value_counts()}")

data.head()

"""#RF1 = using text as input only
#RF2 = text + class of the flights
"""

cv = CountVectorizer(max_features= 3500)
X = cv.fit_transform(corpus).toarray()
Y = data['Sentiment'].values

#splitting 30% testing data
X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size =0.3,random_state =15)
print(f"X train :{X_train.shape}")
print(f"Y train :{Y_train.shape}")
print(f"X test :{X_test.shape}")
print(f"Y test :{Y_test.shape}")

print(f"X train max value: {X_train.max()}")
print(f"X test max value: {X_test.max()}")

scaler = MinMaxScaler()

X_train_scl = scaler.fit_transform(X_train)
X_test_scl = scaler.transform(X_test)

model_rf = RandomForestClassifier()
model_rf.fit(X_train_scl, Y_train)
print("Training Accuracy :", model_rf.score(X_train_scl, Y_train))
print("Testing Accuracy :", model_rf.score(X_test_scl, Y_test))

predictions = model_rf.predict(X_test_scl)

# Compute confusion matrix
conf_matrix = confusion_matrix(Y_test, predictions)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""## Confusion matrix of RandomForestClassifier

The confusion matrix shows the following:

- **True Positives (TP):** 193 - These are the cases where the model correctly predicted a positive sentiment.
- **False Negatives (FN):** 18 - These are the cases where the model incorrectly predicted a negative sentiment when the sentiment was actually positive.
- **False Positives (FP):** 10 - These are the cases where the model incorrectly predicted a positive sentiment when the sentiment was actually negative.
- **True Negatives (TN):** 179 - These are the cases where the model correctly predicted a negative sentiment.

From this confusion matrix, we can learn the following:

- The model is good at predicting positive sentiment, with a high TP rate of 91.4%.
- The model is also good at predicting negative sentiment, with a high TN rate of 94.7%.
- The model is more likely to make false positive predictions (incorrectly predicting a positive sentiment) than false negative predictions (incorrectly predicting a negative sentiment).
- Overall, the model has a good accuracy of 92.9%, but there is room for improvement in reducing the number of false positive predictions.
"""

# Predict sentiment on the test set
sentiment_predictions = model_rf.predict(X_test_scl)

# Count the number of positive and negative predictions
positive_count = np.sum(sentiment_predictions == 1)
negative_count = np.sum(sentiment_predictions == 0)

# Create a bar plot
plt.bar(['Positive', 'Negative'], [positive_count, negative_count], color=['green', 'red'])
plt.title('Sentiment Analysis Results')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

model_dt = DecisionTreeClassifier()
model_dt.fit(X_train_scl, Y_train)
print("Training Accuracy :", model_dt.score(X_train_scl, Y_train))
print("Testing Accuracy :", model_dt.score(X_test_scl, Y_test))

predictions = model_dt.predict(X_test_scl)

# Compute confusion matrix
conf_matrix = confusion_matrix(Y_test, predictions)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

